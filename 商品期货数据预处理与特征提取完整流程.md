# å•†å“æœŸè´§æ•°æ®é¢„å¤„ç†ä¸Žç‰¹å¾æå–å®Œæ•´æµç¨‹

---

## ðŸ“Œ çŽ¯å¢ƒå‡†å¤‡
```python
import os
import pandas as pd
import numpy as np
from glob import glob
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import pickle
import time
```

---

## ðŸ“ è·¯å¾„é…ç½®
```python
# ä¿®æ”¹ä¸ºæ‚¨çš„å®žé™…è·¯å¾„
RAW_DATA_PATH = "data/raw/"  # åŽŸå§‹CSVæ–‡ä»¶å¤¹è·¯å¾„
PROCESSED_PATH = "data/processed/"  # å¤„ç†ç»“æžœä¿å­˜è·¯å¾„
SCALERS_PATH = "data/scalers/"  # æ ‡å‡†åŒ–å™¨ä¿å­˜è·¯å¾„
```

---

## ðŸ§¹ **Step 1: æ•°æ®åˆå¹¶**
```python
%%time
def merge_csv_files():
    if os.path.exists(f"{PROCESSED_PATH}merged_data.csv"):
        print("âœ… å·²å­˜åœ¨åˆå¹¶æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}merged_data.csv")
    
    print("ðŸ”„ æ­£åœ¨åˆå¹¶CSVæ–‡ä»¶...")
    file_paths = sorted(glob(f"{RAW_DATA_PATH}*.csv"))
    dfs = []
    
    for file in file_paths:
        df = pd.read_csv(file)
        date_str = os.path.basename(file).replace(".csv", "")
        df['datetime'] = pd.to_datetime(date_str + " " + df['datetime'])
        dfs.append(df)
    
    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df.to_csv(f"{PROCESSED_PATH}merged_data.csv", index=False)
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}merged_data.csv")
    return merged_df

df = merge_csv_files()
```

---

## ðŸ§¼ **Step 2: ç¼ºå¤±å€¼å¤„ç†**
```python
%%time
def handle_missing_values(df):
    if os.path.exists(f"{PROCESSED_PATH}cleaned_missing.csv"):
        print("âœ… å·²å­˜åœ¨ç¼ºå¤±å€¼å¤„ç†ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}cleaned_missing.csv")
    
    print("ðŸ”„ å¤„ç†ç¼ºå¤±å€¼...")
    df.fillna(method='ffill', inplace=True)
    df.dropna(inplace=True)
    df.to_csv(f"{PROCESSED_PATH}cleaned_missing.csv", index=False)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}cleaned_missing.csv")
    return df

df = handle_missing_values(df)
```

---

## ðŸš« **Step 3: é‡å¤å€¼å¤„ç†**
```python
%%time
def remove_duplicates(df):
    if os.path.exists(f"{PROCESSED_PATH}cleaned_duplicates.csv"):
        print("âœ… å·²å­˜åœ¨åŽ»é‡ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}cleaned_duplicates.csv")
    
    print("ðŸ”„ åŽ»é™¤é‡å¤å€¼...")
    df.drop_duplicates(subset=['datetime'], keep='last', inplace=True)
    df.to_csv(f"{PROCESSED_PATH}cleaned_duplicates.csv", index=False)
    print(f"âœ… åŽ»é‡å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}cleaned_duplicates.csv")
    return df

df = remove_duplicates(df)
```

---

## â±ï¸ **Step 4: æ—¶é—´æˆ³è¿žç»­æ€§å¤„ç†**
```python
%%time
def fix_time_continuity(df):
    if os.path.exists(f"{PROCESSED_PATH}continuous_time.csv"):
        print("âœ… å·²å­˜åœ¨è¿žç»­æ—¶é—´æˆ³å¤„ç†ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}continuous_time.csv")
    
    print("ðŸ”„ ä¿®å¤æ—¶é—´æˆ³è¿žç»­æ€§...")
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    df = df.asfreq('T')  # 1åˆ†é’Ÿé¢‘çŽ‡
    df.reset_index(inplace=True)
    df.to_csv(f"{PROCESSED_PATH}continuous_time.csv", index=False)
    print(f"âœ… æ—¶é—´æˆ³ä¿®å¤å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}continuous_time.csv")
    return df

df = fix_time_continuity(df)
```

---

## ðŸ“ **Step 5: ä»·æ ¼å½’ä¸€åŒ–ï¼ˆMin-Maxï¼‰**
```python
%%time
def normalize_prices(df):
    if os.path.exists(f"{PROCESSED_PATH}normalized_price.csv"):
        print("âœ… å·²å­˜åœ¨ä»·æ ¼å½’ä¸€åŒ–ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}normalized_price.csv")
    
    print("ðŸ”„ ä»·æ ¼å½’ä¸€åŒ–å¤„ç†...")
    scaler = MinMaxScaler()
    price_cols = ['open', 'high', 'low', 'close']
    df[price_cols] = scaler.fit_transform(df[price_cols])
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    os.makedirs(SCALERS_PATH, exist_ok=True)
    with open(f"{SCALERS_PATH}scaler_price.pkl", 'wb') as f:
        pickle.dump(scaler, f)
    
    df.to_csv(f"{PROCESSED_PATH}normalized_price.csv", index=False)
    print(f"âœ… ä»·æ ¼å½’ä¸€åŒ–å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}normalized_price.csv")
    return df

df = normalize_prices(df)
```

---

## ðŸ”¢ **Step 6: æˆäº¤é‡æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰**
```python
%%time
def normalize_volumes(df):
    if os.path.exists(f"{PROCESSED_PATH}normalized_vol.csv"):
        print("âœ… å·²å­˜åœ¨æˆäº¤é‡æ ‡å‡†åŒ–ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}normalized_vol.csv")
    
    print("ðŸ”„ æˆäº¤é‡æ ‡å‡†åŒ–å¤„ç†...")
    scaler = StandardScaler()
    vol_cols = ['volume', 'openinterest']
    df[vol_cols] = scaler.fit_transform(df[vol_cols])
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    with open(f"{SCALERS_PATH}scaler_vol.pkl", 'wb') as f:
        pickle.dump(scaler, f)
    
    df.to_csv(f"{PROCESSED_PATH}normalized_vol.csv", index=False)
    print(f"âœ… æˆäº¤é‡æ ‡å‡†åŒ–å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}normalized_vol.csv")
    return df

df = normalize_volumes(df)
```

---

## ðŸ“ˆ **Step 7: ç‰¹å¾æå–**
```python
%%time
def extract_features(df):
    if os.path.exists(f"{PROCESSED_PATH}final_data.csv"):
        print("âœ… å·²å­˜åœ¨ç‰¹å¾æå–ç»“æžœï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return pd.read_csv(f"{PROCESSED_PATH}final_data.csv")
    
    print("ðŸ”„ æå–ç‰¹å¾...")
    # æ—¶é—´ç‰¹å¾
    df['hour'] = df['datetime'].dt.hour
    df['minute'] = df['datetime'].dt.minute
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['sin_minute'] = np.sin(2 * np.pi * df['minute'] / 60)
    df['cos_minute'] = np.cos(2 * np.pi * df['minute'] / 60)
    
    # æŠ€æœ¯æŒ‡æ ‡
    df['MA5'] = df['close'].rolling(window=5).mean()
    df['MA20'] = df['close'].rolling(window=20).mean()
    
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    rolling_mean = df['close'].rolling(window=20).mean()
    rolling_std = df['close'].rolling(window=20).std()
    df['Bollinger_High'] = rolling_mean + 2 * rolling_std
    df['Bollinger_Low'] = rolling_mean - 2 * rolling_std
    
    df['price_change_1m'] = df['close'].pct_change()
    df['price_change_5m'] = df['close'].pct_change(periods=5)
    
    df.to_csv(f"{PROCESSED_PATH}final_data.csv", index=False)
    print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}final_data.csv")
    return df

df = extract_features(df)
```

---

## ðŸ§ª **Step 8: ç›‘ç£å­¦ä¹ æ•°æ®é›†æž„å»º**
```python
%%time
def create_dataset(df, look_back=60, target_window=30):
    if os.path.exists(f"{PROCESSED_PATH}dataset.npz"):
        print("âœ… å·²å­˜åœ¨ç›‘ç£å­¦ä¹ æ•°æ®é›†ï¼Œè·³è¿‡è¯¥æ­¥éª¤")
        return np.load(f"{PROCESSED_PATH}dataset.npz")
    
    print("ðŸ”„ æž„å»ºç›‘ç£å­¦ä¹ æ•°æ®é›†...")
    X, Y = [], []
    for i in range(len(df) - look_back - target_window):
        X.append(df.iloc[i:i+look_back].values)
        Y.append((df['close'].iloc[i+look_back+target_window] - df['close'].iloc[i+look_back]) / df['close'].iloc[i+look_back])
    
    X, Y = np.array(X), np.array(Y)
    
    # åˆ’åˆ†æ•°æ®é›†
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
    y_train, y_val, y_test = Y[:train_size], Y[train_size:train_size+val_size], Y[train_size+val_size:]
    
    np.savez(f"{PROCESSED_PATH}dataset.npz", 
             X_train=X_train, X_val=X_val, X_test=X_test,
             y_train=y_train, y_val=y_val, y_test=y_test)
    
    print(f"âœ… æ•°æ®é›†æž„å»ºå®Œæˆï¼Œå·²ä¿å­˜è‡³ {PROCESSED_PATH}dataset.npz")
    return {
        'X_train': X_train, 'y_train': y_train,
        'X_val': X_val, 'y_val': y_val,
        'X_test': X_test, 'y_test': y_test
    }

dataset = create_dataset(df)
```

---

## ðŸ“Š **éªŒè¯æ•°æ®è´¨é‡**
```python
# æŸ¥çœ‹æœ€ç»ˆæ•°æ®å½¢çŠ¶
print("æœ€ç»ˆæ•°æ®å½¢çŠ¶:", df.shape)
print("è®­ç»ƒé›†å½¢çŠ¶:", dataset['X_train'].shape)
print("æµ‹è¯•é›†å½¢çŠ¶:", dataset['X_test'].shape)

# æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®
df.head()
```

---

## ðŸ§¾ **æ‰§è¡Œæ—¶é—´ç»Ÿè®¡**
```python
# è®°å½•å„æ­¥éª¤è€—æ—¶ï¼ˆå¯é€‰ï¼‰
# å¯åœ¨æ¯ä¸ªæ­¥éª¤æ·»åŠ  time.time() è®°å½•å¼€å§‹å’Œç»“æŸæ—¶é—´
```

---

## ðŸ§  **æ³¨æ„äº‹é¡¹**
1. **è·¯å¾„ç®¡ç†**ï¼šç¡®ä¿ `data/raw/` ç›®å½•ä¸‹åŒ…å«æ‰€æœ‰CSVæ–‡ä»¶
2. **å†…å­˜ä¼˜åŒ–**ï¼šå¤„ç†å¤§æ•°æ®æ—¶å»ºè®®ä½¿ç”¨Daskæˆ–åˆ†å—å¤„ç†
3. **å¯æ¢å¤æ€§**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½ä¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æžœæ–‡ä»¶ï¼Œé¿å…é‡å¤è®¡ç®—
4. **æ ‡å‡†åŒ–å™¨**ï¼šä¿å­˜çš„ `scaler_price.pkl` å’Œ `scaler_vol.pkl` å¯ç”¨äºŽé¢„æµ‹æ—¶åå½’ä¸€åŒ–















### **ç»“è®ºï¼šæ··åˆæ¨¡åž‹æ˜¯å¦éœ€è¦æ­£æ€åˆ†å¸ƒï¼Ÿ**

#### **1. æ ¸å¿ƒç»“è®º**
- **ä¸éœ€è¦å¼ºåˆ¶æ­£æ€åˆ†å¸ƒ**ã€‚  
  **Attention-based CNN-LSTM å’Œ XGBoost æ··åˆæ¨¡åž‹**å¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒå½¢çŠ¶ï¼ˆå¦‚æ­£æ€æ€§ï¼‰æ²¡æœ‰ä¸¥æ ¼è¦æ±‚ï¼Œä½†éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
  - **ç‰¹å¾æ ‡å‡†åŒ–**ï¼šå»ºè®®å¯¹è¾“å…¥æ•°æ®è¿›è¡Œ Z-score æˆ– Min-Max æ ‡å‡†åŒ–ï¼Œä»¥åŠ é€Ÿæ¨¡åž‹æ”¶æ•›ã€‚
  - **å¼‚å¸¸å€¼å¤„ç†**ï¼šåŽšå°¾æˆ–åæ€åˆ†å¸ƒå¯èƒ½å¯¼è‡´æ¨¡åž‹ä¸ç¨³å®šï¼Œéœ€é€šè¿‡æ•°æ®æ¸…æ´—æˆ–å˜æ¢ï¼ˆå¦‚å¯¹æ•°å˜æ¢ï¼‰é™ä½Žæžç«¯å€¼å½±å“ã€‚
  - **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿æ‰€æœ‰è¾“å…¥ç‰¹å¾åœ¨åŒä¸€å°ºåº¦ä¸Šï¼ˆå¦‚å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰ã€‚

---

### **2. è¯¦ç»†åˆ†æž**

#### **(1) Attention-based CNN-LSTM éƒ¨åˆ†**
- **æ˜¯å¦éœ€è¦æ­£æ€åˆ†å¸ƒï¼Ÿ**
  - **ä¸éœ€è¦**ã€‚  
    CNN å’Œ LSTM æ˜¯ç¥žç»ç½‘ç»œæ¨¡åž‹ï¼Œå…¶è®­ç»ƒè¿‡ç¨‹åŸºäºŽæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒå½¢çŠ¶ä¸æ•æ„Ÿã€‚åªè¦æ•°æ®ç»è¿‡æ ‡å‡†åŒ–ï¼ˆå¦‚ Z-scoreï¼‰ï¼Œæ¨¡åž‹å³å¯æœ‰æ•ˆå­¦ä¹ ç‰¹å¾ã€‚
  - **ä½†éœ€æ³¨æ„**ï¼š
    - **åŽšå°¾åˆ†å¸ƒï¼ˆHeavy-tailï¼‰**ï¼šæˆäº¤é‡ç­‰é‡‘èžæ•°æ®é€šå¸¸å…·æœ‰åŽšå°¾ç‰¹æ€§ï¼ˆæžç«¯å€¼å¤šï¼‰ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸æˆ–æ”¶æ•›å›°éš¾ã€‚å»ºè®®é€šè¿‡ **Winsorizationï¼ˆç¼©å°¾å¤„ç†ï¼‰** æˆ– **å¯¹æ•°å˜æ¢** é™ä½Žæžç«¯å€¼å½±å“ã€‚
    - **åæ€åˆ†å¸ƒï¼ˆSkewnessï¼‰**ï¼šåæ€æ•°æ®å¯èƒ½å¯¼è‡´æ¨¡åž‹å¯¹é•¿å°¾åŒºåŸŸçš„é¢„æµ‹èƒ½åŠ›ä¸‹é™ã€‚å¯é€šè¿‡ **Box-Cox å˜æ¢** æˆ– **åˆ†ä½æ•°å½’ä¸€åŒ–** æ”¹å–„ã€‚

#### **(2) XGBoost éƒ¨åˆ†**
- **æ˜¯å¦éœ€è¦æ­£æ€åˆ†å¸ƒï¼Ÿ**
  - **å®Œå…¨ä¸éœ€è¦**ã€‚  
    XGBoost æ˜¯åŸºäºŽå†³ç­–æ ‘çš„é›†æˆæ¨¡åž‹ï¼Œå¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒå½¢çŠ¶æ— ä»»ä½•å‡è®¾ã€‚å®ƒé€šè¿‡åˆ†è£‚ç‚¹æœç´¢ï¼ˆSplit Point Searchï¼‰ç›´æŽ¥å»ºæ¨¡æ•°æ®åˆ†å¸ƒï¼Œå› æ­¤ï¼š
    - **æ— éœ€æ ‡å‡†åŒ–**ï¼ˆä½†æŽ¨èæ ‡å‡†åŒ–ä»¥æå‡æ•°å€¼ç¨³å®šæ€§ï¼‰ã€‚
    - **æ— éœ€æ­£æ€æ€§å‡è®¾**ï¼šå³ä½¿æ•°æ®å‘ˆçŽ°åæ€ã€åŽšå°¾æˆ–å¤šå³°åˆ†å¸ƒï¼ŒXGBoost ä»èƒ½æœ‰æ•ˆå»ºæ¨¡ã€‚

#### **(3) Attention æœºåˆ¶çš„å½±å“**
- **æ˜¯å¦ä¾èµ–æ­£æ€åˆ†å¸ƒï¼Ÿ**
  - **ä¸ä¾èµ–**ã€‚  
    Attention æœºåˆ¶é€šè¿‡è®¡ç®—ç‰¹å¾ä¹‹é—´çš„æƒé‡ï¼ˆå¦‚ softmax æˆ–åŠ æƒæ±‚å’Œï¼‰å®žçŽ°ç‰¹å¾é‡è¦æ€§åˆ†é…ï¼Œå…¶æ•ˆæžœå–å†³äºŽç‰¹å¾çš„ç›¸å¯¹å…³ç³»ï¼Œè€Œéžç»å¯¹åˆ†å¸ƒå½¢çŠ¶ã€‚

---

### **3. å®žé™…åº”ç”¨å»ºè®®**

#### **(1) æ•°æ®é¢„å¤„ç†æ­¥éª¤**
1. **æ ‡å‡†åŒ–ï¼ˆZ-score æˆ– Min-Maxï¼‰**ï¼š
   - å¯¹æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼ˆå¦‚ä»·æ ¼ã€æˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡ï¼‰è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç¡®ä¿æ¨¡åž‹è¾“å…¥å°ºåº¦ä¸€è‡´ã€‚
   - ç¤ºä¾‹ä»£ç ï¼š
     ```python
     from sklearn.preprocessing import StandardScaler
     
     scaler = StandardScaler()
     df_scaled = scaler.fit_transform(df)
     ```

2. **å¼‚å¸¸å€¼å¤„ç†**ï¼š
   - **Winsorization**ï¼šå°†è¶…å‡ºé˜ˆå€¼çš„æžå€¼æˆªæ–­åˆ°é˜ˆå€¼èŒƒå›´å†…ã€‚
     ```python
     # æˆªæ–­ä¸Šä¸‹ 1% çš„æžå€¼
     df['volume'] = df['volume'].clip(lower=df['volume'].quantile(0.01), upper=df['volume'].quantile(0.99))
     ```
   - **å¯¹æ•°å˜æ¢**ï¼šé€‚ç”¨äºŽå³åæ•°æ®ï¼ˆå¦‚æˆäº¤é‡ã€ä»·æ ¼ï¼‰ã€‚
     ```python
     df['volume_log'] = np.log1p(df['volume'])
     ```

3. **ç‰¹å¾å·¥ç¨‹**ï¼š
   - æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ï¼ˆå¦‚ RSIã€MACDï¼‰ã€ç»Ÿè®¡ç‰¹å¾ï¼ˆå¦‚æ»šåŠ¨å‡å€¼ã€æ³¢åŠ¨çŽ‡ï¼‰ä»¥å¢žå¼ºæ¨¡åž‹è¡¨è¾¾èƒ½åŠ›ã€‚

#### **(2) æ¨¡åž‹è®­ç»ƒç­–ç•¥**
- **CNN-LSTM éƒ¨åˆ†**ï¼š
  - ä½¿ç”¨æ ‡å‡†åŒ–åŽçš„æ•°æ®è®­ç»ƒï¼Œé¿å…æ¢¯åº¦ä¸ç¨³å®šã€‚
  - è‹¥æ•°æ®åˆ†å¸ƒæžç«¯åæ€ï¼Œå¯ç»“åˆ **æ•°æ®å¢žå¼º**ï¼ˆå¦‚ SMOTEï¼‰æå‡æ¨¡åž‹é²æ£’æ€§ã€‚
- **XGBoost éƒ¨åˆ†**ï¼š
  - ç›´æŽ¥ä½¿ç”¨åŽŸå§‹æ•°æ®æˆ–æ ‡å‡†åŒ–æ•°æ®å‡å¯ã€‚
  - é€šè¿‡ **æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰** é¿å…è¿‡æ‹Ÿåˆã€‚

#### **(3) éªŒè¯æ­£æ€æ€§çš„å½±å“**
- **å®žéªŒå¯¹æ¯”**ï¼š
  1. **Case 1**ï¼šç›´æŽ¥ä½¿ç”¨ Z-score æ ‡å‡†åŒ–æ•°æ®ï¼ˆéžæ­£æ€åˆ†å¸ƒï¼‰ã€‚
  2. **Case 2**ï¼šå¯¹æ•°æ®è¿›è¡Œ Box-Cox å˜æ¢åŽæ ‡å‡†åŒ–ï¼ˆè¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼‰ã€‚
  3. **è¯„ä¼°æŒ‡æ ‡**ï¼šæ¯”è¾ƒä¸¤ç»„æ¨¡åž‹çš„é¢„æµ‹è¯¯å·®ï¼ˆå¦‚ MAEã€RMSEï¼‰ã€‚
  - **ç¤ºä¾‹ä»£ç **ï¼š
    ```python
    from sklearn.metrics import mean_squared_error
    
    # Case 1: Z-score æ ‡å‡†åŒ–
    y_pred_case1 = model.predict(X_test_case1)
    rmse_case1 = mean_squared_error(y_test, y_pred_case1, squared=False)
    
    # Case 2: Box-Cox + Z-score æ ‡å‡†åŒ–
    y_pred_case2 = model.predict(X_test_case2)
    rmse_case2 = mean_squared_error(y_test, y_pred_case2, squared=False)
    ```

---

### **4. çŸ¥è¯†åº“æ”¯æŒ**
- **[1] å’Œ [2]** å¼ºè°ƒæ­£æ€åˆ†å¸ƒçš„ç†è®ºæ¡ä»¶ï¼ˆè¿žç»­æ€§ã€å¯¹ç§°æ€§ã€å•å³°æ€§ï¼‰ï¼Œä½†æœªæåŠå…¶åœ¨æœºå™¨å­¦ä¹ ä¸­çš„å¼ºåˆ¶è¦æ±‚ã€‚
- **[3]** æ˜Žç¡®æŒ‡å‡ºï¼š  
  - **æ­£æ€åˆ†å¸ƒä¸»è¦ç”¨äºŽæ•°æ®é¢„å¤„ç†ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼‰**ï¼Œè€Œéžæ¨¡åž‹æœ¬èº«çš„å‡è®¾ã€‚
  - **æ ‘æ¨¡åž‹ï¼ˆå¦‚ XGBoostï¼‰å¯¹è¾“å…¥åˆ†å¸ƒæ— è¦æ±‚**ï¼Œè€Œç¥žç»ç½‘ç»œï¼ˆå¦‚ CNN-LSTMï¼‰æ›´å…³æ³¨æ•°æ®å°ºåº¦ä¸€è‡´æ€§ã€‚

---

### **5. æ€»ç»“**
| æ¨¡åž‹ç»„ä»¶           | æ˜¯å¦éœ€è¦æ­£æ€åˆ†å¸ƒ | å…³é”®è¦æ±‚                 |
| ------------------ | ---------------- | ------------------------ |
| **CNN-LSTM**       | å¦               | æ ‡å‡†åŒ–ã€å¼‚å¸¸å€¼å¤„ç†       |
| **XGBoost**        | å¦               | æ— éœ€æ ‡å‡†åŒ–ï¼ˆæŽ¨èæ ‡å‡†åŒ–ï¼‰ |
| **Attention æœºåˆ¶** | å¦               | ç‰¹å¾ç›¸å…³æ€§å»ºæ¨¡           |

**æœ€ç»ˆå»ºè®®**ï¼š  
- **æ— éœ€å¼ºåˆ¶æ­£æ€åˆ†å¸ƒ**ï¼Œä½†éœ€ç¡®ä¿æ•°æ®æ ‡å‡†åŒ–å’Œå¼‚å¸¸å€¼å¤„ç†ã€‚  
- é€šè¿‡å®žéªŒéªŒè¯ä¸åŒé¢„å¤„ç†æ–¹æ³•å¯¹æ¨¡åž‹æ€§èƒ½çš„å½±å“ï¼Œé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚





åœ¨æž„å»ºåŸºäºŽ **ARIMA é¢„å¤„ç† + Attention-based CNN-LSTM + XGBoost** çš„è‚¡ç¥¨é¢„æµ‹æ¨¡åž‹æ—¶ï¼Œ**ARIMA é¢„å¤„ç†å’Œç‰¹å¾æå–çš„é¡ºåºåº”éµå¾ªä»¥ä¸‹é€»è¾‘**ï¼š

---

### **1. æ ¸å¿ƒåŽŸåˆ™**
- **ARIMA é¢„å¤„ç†æ˜¯ç‰¹å¾å·¥ç¨‹çš„ä¸€éƒ¨åˆ†**ï¼šARIMA é€šè¿‡å·®åˆ†æ“ä½œç”Ÿæˆå¹³ç¨³åºåˆ—ï¼Œæœ¬è´¨ä¸Šæ˜¯æå–æ—¶é—´åºåˆ—çš„çº¿æ€§æ¨¡å¼ï¼ˆå¦‚è¶‹åŠ¿ã€å­£èŠ‚æ€§ï¼‰ï¼Œå±žäºŽ **ç‰¹å¾æå–** çš„èŒƒç•´ã€‚
- **ç‰¹å¾æå–éœ€è¦åŸºäºŽé¢„å¤„ç†åŽçš„æ•°æ®**ï¼šåŽŸå§‹æ•°æ®ï¼ˆå¦‚è‚¡ç¥¨ä»·æ ¼ï¼‰é€šå¸¸éžå¹³ç¨³ï¼Œç›´æŽ¥æå–ç‰¹å¾å¯èƒ½å¯¼è‡´æ— æ•ˆæˆ–è¯¯å¯¼æ€§ç»“æžœã€‚å› æ­¤ï¼Œ**ARIMA é¢„å¤„ç†åº”ä¼˜å…ˆäºŽç‰¹å¾æå–**ã€‚

---

### **2. æ­£ç¡®çš„æµç¨‹é¡ºåº**
#### **(1) æ•°æ®é¢„å¤„ç†**
1. **åŽŸå§‹æ•°æ®æ¸…æ´—**  
   - åŽ»é™¤ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼ˆå¦‚è‚¡ä»·çªå˜æˆ–ç¼ºå¤±æ—¥ï¼‰ã€‚
   - æ ‡å‡†åŒ–ï¼ˆZ-score æˆ– Min-Maxï¼‰ä»¥é€‚é…æ¨¡åž‹è¾“å…¥è¦æ±‚ã€‚
   - ç¤ºä¾‹ä»£ç ï¼š
     ```python
     from sklearn.preprocessing import StandardScaler
     
     scaler = StandardScaler()
     df_scaled = scaler.fit_transform(df)
     ```

2. **ARIMA é¢„å¤„ç†ï¼ˆç”Ÿæˆå¹³ç¨³åºåˆ—ï¼‰**  
   - **ç›®çš„**ï¼šé€šè¿‡å·®åˆ†æ“ä½œæ¶ˆé™¤è¶‹åŠ¿å’Œå­£èŠ‚æ€§ï¼Œç”Ÿæˆå¹³ç¨³åºåˆ—ï¼ˆå¦‚æ”¶ç›ŠçŽ‡åºåˆ—ï¼‰ã€‚  
   - **æ­¥éª¤**ï¼š  
     - å¹³ç¨³æ€§æ£€éªŒï¼ˆADF æ£€éªŒï¼‰ â†’ å·®åˆ†å¤„ç†ï¼ˆç¡®å®šå·®åˆ†é˜¶æ•° `d`ï¼‰ â†’ æå–æ®‹å·®ï¼ˆå¹³ç¨³åºåˆ—ï¼‰ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from statsmodels.tsa.arima.model import ARIMA
     
     model = ARIMA(df['Close'], order=(p, d, q))  # p=è‡ªå›žå½’é˜¶æ•°, d=å·®åˆ†é˜¶æ•°, q=ç§»åŠ¨å¹³å‡é˜¶æ•°
     results = model.fit()
     stationary_series = results.resid  # å·®åˆ†åŽçš„å¹³ç¨³åºåˆ—
     ```

3. **ä¿å­˜é¢„å¤„ç†ç»“æžœ**  
   - å°† ARIMA ç”Ÿæˆçš„å¹³ç¨³åºåˆ—ä¸ŽåŽŸå§‹æ•°æ®æ‹¼æŽ¥ï¼Œä½œä¸ºåŽç»­ç‰¹å¾æå–çš„è¾“å…¥ã€‚

#### **(2) ç‰¹å¾æå–**
1. **ä»Žå¹³ç¨³åºåˆ—ä¸­æå–ç»Ÿè®¡ç‰¹å¾**  
   - **æŠ€æœ¯æŒ‡æ ‡**ï¼šRSIã€MACDã€å¸ƒæž—å¸¦ç­‰ï¼ˆé€šè¿‡ `ta` åº“å®žçŽ°ï¼‰ã€‚  
   - **ç»Ÿè®¡ç‰¹å¾**ï¼šæ»šåŠ¨å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ç­‰ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     import ta
     
     df['rsi'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()
     df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()
     ```

2. **ç»“åˆåŽŸå§‹æ•°æ®ä¸Ž ARIMA æ®‹å·®**  
   - å°†å¹³ç¨³åºåˆ—ï¼ˆARIMA æ®‹å·®ï¼‰ä¸ŽåŽŸå§‹æ•°æ®ï¼ˆå¦‚å¼€ç›˜ä»·ã€æˆäº¤é‡ï¼‰æ‹¼æŽ¥ï¼Œå½¢æˆå¤šç»´ç‰¹å¾ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     features = pd.concat([df, stationary_series], axis=1)
     ```

3. **é™ç»´ï¼ˆå¯é€‰ï¼‰**  
   - ä½¿ç”¨ PCA æˆ– t-SNE é™ä½Žç‰¹å¾ç»´åº¦ï¼Œå‡å°‘å†—ä½™ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from sklearn.decomposition import PCA
     
     pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
     X_pca = pca.fit_transform(features)
     ```

#### **(3) æ¨¡åž‹è®­ç»ƒä¸Žé›†æˆ**
1. **Attention-based CNN-LSTM æ¨¡åž‹**  
   - è¾“å…¥ä¸º 3D å¼ é‡ï¼ˆæ ·æœ¬æ•° Ã— æ—¶é—´æ­¥ Ã— ç‰¹å¾æ•°ï¼‰ï¼Œåˆ©ç”¨å·ç§¯æå–å±€éƒ¨ç‰¹å¾ï¼ŒLSTM æ•æ‰é•¿æœŸä¾èµ–ï¼ŒAttention æœºåˆ¶åŠ æƒé‡è¦ç‰¹å¾ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from tensorflow.keras.models import Model
     from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, GlobalAveragePooling1D, Attention
     
     inputs = Input(shape=(time_steps, num_features))
     x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
     x = LSTM(units=64, return_sequences=True)(x)
     attention = Attention()([x, x])  # è‡ªæ³¨æ„åŠ›æœºåˆ¶
     x = GlobalAveragePooling1D()(attention)
     outputs = Dense(1)(x)
     
     model = Model(inputs=inputs, outputs=outputs)
     model.compile(optimizer='adam', loss='mse')
     ```

2. **XGBoost å¾®è°ƒ**  
   - è¾“å…¥ä¸ºæ‰å¹³åŒ–åŽçš„ç‰¹å¾å‘é‡ï¼ˆæ ·æœ¬æ•° Ã— ç‰¹å¾æ•°ï¼‰ï¼Œé€šè¿‡å†³ç­–æ ‘è¿›ä¸€æ­¥æŒ–æŽ˜å¤šæœŸä¿¡æ¯ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from xgboost import XGBRegressor
     
     xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)
     xgb_model.fit(X_train_flat, y_train)
     ```

3. **æ¨¡åž‹é›†æˆ**  
   - å°† CNN-LSTM å’Œ XGBoost çš„é¢„æµ‹ç»“æžœé€šè¿‡åŠ æƒå¹³å‡æˆ– Stacking èžåˆã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from sklearn.ensemble import StackingRegressor
     
     stack_model = StackingRegressor(
         estimators=[
             ('cnn_lstm', cnn_lstm_model),
             ('xgb', xgb_model)
         ],
         final_estimator=LinearRegression()
     )
     ```

---

### **3. å…³é”®æ³¨æ„äº‹é¡¹**
1. **ARIMA é¢„å¤„ç†çš„å¿…è¦æ€§**  
   - **åŽŸå§‹æ•°æ®éžå¹³ç¨³æ—¶å¿…é¡»è¿›è¡Œ ARIMA é¢„å¤„ç†**ï¼ˆå¦‚è‚¡ç¥¨ä»·æ ¼å…·æœ‰è¶‹åŠ¿æ€§ï¼‰ã€‚  
   - **å¹³ç¨³åºåˆ—èƒ½æå‡æ¨¡åž‹é²æ£’æ€§**ï¼šCNN-LSTM å’Œ XGBoost å¯¹éžå¹³ç¨³æ•°æ®çš„æ‹Ÿåˆèƒ½åŠ›æœ‰é™ï¼ŒARIMA å¯é™ä½Žå™ªå£°å¹²æ‰°ã€‚

2. **ç‰¹å¾æå–çš„å¤šæ ·æ€§**  
   - **ç»“åˆçº¿æ€§ï¼ˆARIMA æ®‹å·®ï¼‰ä¸Žéžçº¿æ€§ç‰¹å¾ï¼ˆæŠ€æœ¯æŒ‡æ ‡ï¼‰**ï¼šARIMA æå–çº¿æ€§æ¨¡å¼ï¼ŒæŠ€æœ¯æŒ‡æ ‡æ•æ‰å¸‚åœºè¡Œä¸ºï¼ˆå¦‚ RSI è¡¡é‡è¶…ä¹°/è¶…å–ï¼‰ã€‚  
   - **é¿å…è¿‡åº¦ä¾èµ–å•ä¸€ç‰¹å¾**ï¼šå¤šç»´åº¦ç‰¹å¾ï¼ˆå¦‚ä»·æ ¼ã€æˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡ï¼‰å¯æé«˜æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚

3. **å‚æ•°è°ƒä¼˜ä¸ŽéªŒè¯**  
   - **ARIMA å‚æ•°é€‰æ‹©**ï¼šé€šè¿‡ ACF/PACF å›¾æˆ– AIC/BIC å‡†åˆ™ç¡®å®š `p`ã€`d`ã€`q`ã€‚  
   - **æ¨¡åž‹éªŒè¯**ï¼šä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆ`TimeSeriesSplit`ï¼‰é¿å…ä¿¡æ¯æ³„éœ²ã€‚  
   - **ç¤ºä¾‹ä»£ç **ï¼š
     ```python
     from sklearn.model_selection import TimeSeriesSplit
     
     tscv = TimeSeriesSplit(n_splits=5)
     for train_index, test_index in tscv.split(X):
         X_train, X_test = X[train_index], X[test_index]
         y_train, y_test = y[train_index], y[test_index]
     ```

---

### **4. æ€»ç»“**
| **æ­¥éª¤**          | **æ“ä½œ**                                                     | **å·¥å…·/æ–¹æ³•**                         |
| ----------------- | ------------------------------------------------------------ | ------------------------------------- |
| **1. æ•°æ®é¢„å¤„ç†** | åŽ»é™¤å¼‚å¸¸å€¼ã€æ ‡å‡†åŒ–ã€ARIMA å·®åˆ†ç”Ÿæˆå¹³ç¨³åºåˆ—                   | `StandardScaler`, `statsmodels.ARIMA` |
| **2. ç‰¹å¾æå–**   | æå–æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSIã€MACDï¼‰ã€ç»Ÿè®¡ç‰¹å¾ï¼ˆæ»šåŠ¨å‡å€¼ï¼‰ã€ç»“åˆ ARIMA æ®‹å·® | `ta`, `pandas.rolling`                |
| **3. æ¨¡åž‹è®­ç»ƒ**   | CNN-LSTM ç¼–ç æ—¶é—´åºåˆ—ï¼ŒXGBoost å¾®è°ƒå¤šæœŸä¿¡æ¯ï¼ŒStacking é›†æˆé¢„æµ‹ç»“æžœ | `TensorFlow`, `XGBoost`, `sklearn`    |

**æœ€ç»ˆå»ºè®®**ï¼š  
- **å…ˆè¿›è¡Œ ARIMA é¢„å¤„ç†**ï¼Œå°†åŽŸå§‹æ•°æ®è½¬æ¢ä¸ºå¹³ç¨³åºåˆ—ï¼Œå†ç»“åˆæŠ€æœ¯æŒ‡æ ‡å’Œç»Ÿè®¡ç‰¹å¾è¿›è¡Œæå–ã€‚  
- **ç¡®ä¿ ARIMA æ®‹å·®ä¸ŽåŽŸå§‹æ•°æ®æ‹¼æŽ¥åŽè¾“å…¥æ¨¡åž‹**ï¼Œå……åˆ†å‘æŒ¥çº¿æ€§ä¸Žéžçº¿æ€§ç‰¹å¾çš„äº’è¡¥æ€§ã€‚