# 商品期货数据预处理与特征提取完整流程

---

## 📌 环境准备
```python
import os
import pandas as pd
import numpy as np
from glob import glob
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import pickle
import time
```

---

## 📁 路径配置
```python
# 修改为您的实际路径
RAW_DATA_PATH = "data/raw/"  # 原始CSV文件夹路径
PROCESSED_PATH = "data/processed/"  # 处理结果保存路径
SCALERS_PATH = "data/scalers/"  # 标准化器保存路径
```

---

## 🧹 **Step 1: 数据合并**
```python
%%time
def merge_csv_files():
    if os.path.exists(f"{PROCESSED_PATH}merged_data.csv"):
        print("✅ 已存在合并文件，跳过合并步骤")
        return pd.read_csv(f"{PROCESSED_PATH}merged_data.csv")
    
    print("🔄 正在合并CSV文件...")
    file_paths = sorted(glob(f"{RAW_DATA_PATH}*.csv"))
    dfs = []
    
    for file in file_paths:
        df = pd.read_csv(file)
        date_str = os.path.basename(file).replace(".csv", "")
        df['datetime'] = pd.to_datetime(date_str + " " + df['datetime'])
        dfs.append(df)
    
    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df.to_csv(f"{PROCESSED_PATH}merged_data.csv", index=False)
    print(f"✅ 合并完成，已保存至 {PROCESSED_PATH}merged_data.csv")
    return merged_df

df = merge_csv_files()
```

---

## 🧼 **Step 2: 缺失值处理**
```python
%%time
def handle_missing_values(df):
    if os.path.exists(f"{PROCESSED_PATH}cleaned_missing.csv"):
        print("✅ 已存在缺失值处理结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}cleaned_missing.csv")
    
    print("🔄 处理缺失值...")
    df.fillna(method='ffill', inplace=True)
    df.dropna(inplace=True)
    df.to_csv(f"{PROCESSED_PATH}cleaned_missing.csv", index=False)
    print(f"✅ 处理完成，已保存至 {PROCESSED_PATH}cleaned_missing.csv")
    return df

df = handle_missing_values(df)
```

---

## 🚫 **Step 3: 重复值处理**
```python
%%time
def remove_duplicates(df):
    if os.path.exists(f"{PROCESSED_PATH}cleaned_duplicates.csv"):
        print("✅ 已存在去重结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}cleaned_duplicates.csv")
    
    print("🔄 去除重复值...")
    df.drop_duplicates(subset=['datetime'], keep='last', inplace=True)
    df.to_csv(f"{PROCESSED_PATH}cleaned_duplicates.csv", index=False)
    print(f"✅ 去重完成，已保存至 {PROCESSED_PATH}cleaned_duplicates.csv")
    return df

df = remove_duplicates(df)
```

---

## ⏱️ **Step 4: 时间戳连续性处理**
```python
%%time
def fix_time_continuity(df):
    if os.path.exists(f"{PROCESSED_PATH}continuous_time.csv"):
        print("✅ 已存在连续时间戳处理结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}continuous_time.csv")
    
    print("🔄 修复时间戳连续性...")
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    df = df.asfreq('T')  # 1分钟频率
    df.reset_index(inplace=True)
    df.to_csv(f"{PROCESSED_PATH}continuous_time.csv", index=False)
    print(f"✅ 时间戳修复完成，已保存至 {PROCESSED_PATH}continuous_time.csv")
    return df

df = fix_time_continuity(df)
```

---

## 📏 **Step 5: 价格归一化（Min-Max）**
```python
%%time
def normalize_prices(df):
    if os.path.exists(f"{PROCESSED_PATH}normalized_price.csv"):
        print("✅ 已存在价格归一化结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}normalized_price.csv")
    
    print("🔄 价格归一化处理...")
    scaler = MinMaxScaler()
    price_cols = ['open', 'high', 'low', 'close']
    df[price_cols] = scaler.fit_transform(df[price_cols])
    
    # 保存标准化器
    os.makedirs(SCALERS_PATH, exist_ok=True)
    with open(f"{SCALERS_PATH}scaler_price.pkl", 'wb') as f:
        pickle.dump(scaler, f)
    
    df.to_csv(f"{PROCESSED_PATH}normalized_price.csv", index=False)
    print(f"✅ 价格归一化完成，已保存至 {PROCESSED_PATH}normalized_price.csv")
    return df

df = normalize_prices(df)
```

---

## 🔢 **Step 6: 成交量标准化（Z-score）**
```python
%%time
def normalize_volumes(df):
    if os.path.exists(f"{PROCESSED_PATH}normalized_vol.csv"):
        print("✅ 已存在成交量标准化结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}normalized_vol.csv")
    
    print("🔄 成交量标准化处理...")
    scaler = StandardScaler()
    vol_cols = ['volume', 'openinterest']
    df[vol_cols] = scaler.fit_transform(df[vol_cols])
    
    # 保存标准化器
    with open(f"{SCALERS_PATH}scaler_vol.pkl", 'wb') as f:
        pickle.dump(scaler, f)
    
    df.to_csv(f"{PROCESSED_PATH}normalized_vol.csv", index=False)
    print(f"✅ 成交量标准化完成，已保存至 {PROCESSED_PATH}normalized_vol.csv")
    return df

df = normalize_volumes(df)
```

---

## 📈 **Step 7: 特征提取**
```python
%%time
def extract_features(df):
    if os.path.exists(f"{PROCESSED_PATH}final_data.csv"):
        print("✅ 已存在特征提取结果，跳过该步骤")
        return pd.read_csv(f"{PROCESSED_PATH}final_data.csv")
    
    print("🔄 提取特征...")
    # 时间特征
    df['hour'] = df['datetime'].dt.hour
    df['minute'] = df['datetime'].dt.minute
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['sin_minute'] = np.sin(2 * np.pi * df['minute'] / 60)
    df['cos_minute'] = np.cos(2 * np.pi * df['minute'] / 60)
    
    # 技术指标
    df['MA5'] = df['close'].rolling(window=5).mean()
    df['MA20'] = df['close'].rolling(window=20).mean()
    
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    rolling_mean = df['close'].rolling(window=20).mean()
    rolling_std = df['close'].rolling(window=20).std()
    df['Bollinger_High'] = rolling_mean + 2 * rolling_std
    df['Bollinger_Low'] = rolling_mean - 2 * rolling_std
    
    df['price_change_1m'] = df['close'].pct_change()
    df['price_change_5m'] = df['close'].pct_change(periods=5)
    
    df.to_csv(f"{PROCESSED_PATH}final_data.csv", index=False)
    print(f"✅ 特征提取完成，已保存至 {PROCESSED_PATH}final_data.csv")
    return df

df = extract_features(df)
```

---

## 🧪 **Step 8: 监督学习数据集构建**
```python
%%time
def create_dataset(df, look_back=60, target_window=30):
    if os.path.exists(f"{PROCESSED_PATH}dataset.npz"):
        print("✅ 已存在监督学习数据集，跳过该步骤")
        return np.load(f"{PROCESSED_PATH}dataset.npz")
    
    print("🔄 构建监督学习数据集...")
    X, Y = [], []
    for i in range(len(df) - look_back - target_window):
        X.append(df.iloc[i:i+look_back].values)
        Y.append((df['close'].iloc[i+look_back+target_window] - df['close'].iloc[i+look_back]) / df['close'].iloc[i+look_back])
    
    X, Y = np.array(X), np.array(Y)
    
    # 划分数据集
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
    y_train, y_val, y_test = Y[:train_size], Y[train_size:train_size+val_size], Y[train_size+val_size:]
    
    np.savez(f"{PROCESSED_PATH}dataset.npz", 
             X_train=X_train, X_val=X_val, X_test=X_test,
             y_train=y_train, y_val=y_val, y_test=y_test)
    
    print(f"✅ 数据集构建完成，已保存至 {PROCESSED_PATH}dataset.npz")
    return {
        'X_train': X_train, 'y_train': y_train,
        'X_val': X_val, 'y_val': y_val,
        'X_test': X_test, 'y_test': y_test
    }

dataset = create_dataset(df)
```

---

## 📊 **验证数据质量**
```python
# 查看最终数据形状
print("最终数据形状:", df.shape)
print("训练集形状:", dataset['X_train'].shape)
print("测试集形状:", dataset['X_test'].shape)

# 查看前几行数据
df.head()
```

---

## 🧾 **执行时间统计**
```python
# 记录各步骤耗时（可选）
# 可在每个步骤添加 time.time() 记录开始和结束时间
```

---

## 🧠 **注意事项**
1. **路径管理**：确保 `data/raw/` 目录下包含所有CSV文件
2. **内存优化**：处理大数据时建议使用Dask或分块处理
3. **可恢复性**：每个步骤都会检查是否已有结果文件，避免重复计算
4. **标准化器**：保存的 `scaler_price.pkl` 和 `scaler_vol.pkl` 可用于预测时反归一化















### **结论：混合模型是否需要正态分布？**

#### **1. 核心结论**
- **不需要强制正态分布**。  
  **Attention-based CNN-LSTM 和 XGBoost 混合模型**对输入数据的分布形状（如正态性）没有严格要求，但需要满足以下条件：
  - **特征标准化**：建议对输入数据进行 Z-score 或 Min-Max 标准化，以加速模型收敛。
  - **异常值处理**：厚尾或偏态分布可能导致模型不稳定，需通过数据清洗或变换（如对数变换）降低极端值影响。
  - **数据一致性**：确保所有输入特征在同一尺度上（如均值为 0，标准差为 1）。

---

### **2. 详细分析**

#### **(1) Attention-based CNN-LSTM 部分**
- **是否需要正态分布？**
  - **不需要**。  
    CNN 和 LSTM 是神经网络模型，其训练过程基于梯度下降优化，对输入数据的分布形状不敏感。只要数据经过标准化（如 Z-score），模型即可有效学习特征。
  - **但需注意**：
    - **厚尾分布（Heavy-tail）**：成交量等金融数据通常具有厚尾特性（极端值多），可能导致梯度爆炸或收敛困难。建议通过 **Winsorization（缩尾处理）** 或 **对数变换** 降低极端值影响。
    - **偏态分布（Skewness）**：偏态数据可能导致模型对长尾区域的预测能力下降。可通过 **Box-Cox 变换** 或 **分位数归一化** 改善。

#### **(2) XGBoost 部分**
- **是否需要正态分布？**
  - **完全不需要**。  
    XGBoost 是基于决策树的集成模型，对输入数据的分布形状无任何假设。它通过分裂点搜索（Split Point Search）直接建模数据分布，因此：
    - **无需标准化**（但推荐标准化以提升数值稳定性）。
    - **无需正态性假设**：即使数据呈现偏态、厚尾或多峰分布，XGBoost 仍能有效建模。

#### **(3) Attention 机制的影响**
- **是否依赖正态分布？**
  - **不依赖**。  
    Attention 机制通过计算特征之间的权重（如 softmax 或加权求和）实现特征重要性分配，其效果取决于特征的相对关系，而非绝对分布形状。

---

### **3. 实际应用建议**

#### **(1) 数据预处理步骤**
1. **标准化（Z-score 或 Min-Max）**：
   - 对所有输入特征（如价格、成交量、技术指标）进行标准化，确保模型输入尺度一致。
   - 示例代码：
     ```python
     from sklearn.preprocessing import StandardScaler
     
     scaler = StandardScaler()
     df_scaled = scaler.fit_transform(df)
     ```

2. **异常值处理**：
   - **Winsorization**：将超出阈值的极值截断到阈值范围内。
     ```python
     # 截断上下 1% 的极值
     df['volume'] = df['volume'].clip(lower=df['volume'].quantile(0.01), upper=df['volume'].quantile(0.99))
     ```
   - **对数变换**：适用于右偏数据（如成交量、价格）。
     ```python
     df['volume_log'] = np.log1p(df['volume'])
     ```

3. **特征工程**：
   - 添加技术指标（如 RSI、MACD）、统计特征（如滚动均值、波动率）以增强模型表达能力。

#### **(2) 模型训练策略**
- **CNN-LSTM 部分**：
  - 使用标准化后的数据训练，避免梯度不稳定。
  - 若数据分布极端偏态，可结合 **数据增强**（如 SMOTE）提升模型鲁棒性。
- **XGBoost 部分**：
  - 直接使用原始数据或标准化数据均可。
  - 通过 **早停法（Early Stopping）** 避免过拟合。

#### **(3) 验证正态性的影响**
- **实验对比**：
  1. **Case 1**：直接使用 Z-score 标准化数据（非正态分布）。
  2. **Case 2**：对数据进行 Box-Cox 变换后标准化（近似正态分布）。
  3. **评估指标**：比较两组模型的预测误差（如 MAE、RMSE）。
  - **示例代码**：
    ```python
    from sklearn.metrics import mean_squared_error
    
    # Case 1: Z-score 标准化
    y_pred_case1 = model.predict(X_test_case1)
    rmse_case1 = mean_squared_error(y_test, y_pred_case1, squared=False)
    
    # Case 2: Box-Cox + Z-score 标准化
    y_pred_case2 = model.predict(X_test_case2)
    rmse_case2 = mean_squared_error(y_test, y_pred_case2, squared=False)
    ```

---

### **4. 知识库支持**
- **[1] 和 [2]** 强调正态分布的理论条件（连续性、对称性、单峰性），但未提及其在机器学习中的强制要求。
- **[3]** 明确指出：  
  - **正态分布主要用于数据预处理（如标准化）**，而非模型本身的假设。
  - **树模型（如 XGBoost）对输入分布无要求**，而神经网络（如 CNN-LSTM）更关注数据尺度一致性。

---

### **5. 总结**
| 模型组件           | 是否需要正态分布 | 关键要求                 |
| ------------------ | ---------------- | ------------------------ |
| **CNN-LSTM**       | 否               | 标准化、异常值处理       |
| **XGBoost**        | 否               | 无需标准化（推荐标准化） |
| **Attention 机制** | 否               | 特征相关性建模           |

**最终建议**：  
- **无需强制正态分布**，但需确保数据标准化和异常值处理。  
- 通过实验验证不同预处理方法对模型性能的影响，选择最优方案。





在构建基于 **ARIMA 预处理 + Attention-based CNN-LSTM + XGBoost** 的股票预测模型时，**ARIMA 预处理和特征提取的顺序应遵循以下逻辑**：

---

### **1. 核心原则**
- **ARIMA 预处理是特征工程的一部分**：ARIMA 通过差分操作生成平稳序列，本质上是提取时间序列的线性模式（如趋势、季节性），属于 **特征提取** 的范畴。
- **特征提取需要基于预处理后的数据**：原始数据（如股票价格）通常非平稳，直接提取特征可能导致无效或误导性结果。因此，**ARIMA 预处理应优先于特征提取**。

---

### **2. 正确的流程顺序**
#### **(1) 数据预处理**
1. **原始数据清洗**  
   - 去除缺失值、异常值（如股价突变或缺失日）。
   - 标准化（Z-score 或 Min-Max）以适配模型输入要求。
   - 示例代码：
     ```python
     from sklearn.preprocessing import StandardScaler
     
     scaler = StandardScaler()
     df_scaled = scaler.fit_transform(df)
     ```

2. **ARIMA 预处理（生成平稳序列）**  
   - **目的**：通过差分操作消除趋势和季节性，生成平稳序列（如收益率序列）。  
   - **步骤**：  
     - 平稳性检验（ADF 检验） → 差分处理（确定差分阶数 `d`） → 提取残差（平稳序列）。  
   - **示例代码**：
     ```python
     from statsmodels.tsa.arima.model import ARIMA
     
     model = ARIMA(df['Close'], order=(p, d, q))  # p=自回归阶数, d=差分阶数, q=移动平均阶数
     results = model.fit()
     stationary_series = results.resid  # 差分后的平稳序列
     ```

3. **保存预处理结果**  
   - 将 ARIMA 生成的平稳序列与原始数据拼接，作为后续特征提取的输入。

#### **(2) 特征提取**
1. **从平稳序列中提取统计特征**  
   - **技术指标**：RSI、MACD、布林带等（通过 `ta` 库实现）。  
   - **统计特征**：滚动均值、标准差、最大值、最小值等。  
   - **示例代码**：
     ```python
     import ta
     
     df['rsi'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()
     df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()
     ```

2. **结合原始数据与 ARIMA 残差**  
   - 将平稳序列（ARIMA 残差）与原始数据（如开盘价、成交量）拼接，形成多维特征。  
   - **示例代码**：
     ```python
     features = pd.concat([df, stationary_series], axis=1)
     ```

3. **降维（可选）**  
   - 使用 PCA 或 t-SNE 降低特征维度，减少冗余。  
   - **示例代码**：
     ```python
     from sklearn.decomposition import PCA
     
     pca = PCA(n_components=0.95)  # 保留95%方差
     X_pca = pca.fit_transform(features)
     ```

#### **(3) 模型训练与集成**
1. **Attention-based CNN-LSTM 模型**  
   - 输入为 3D 张量（样本数 × 时间步 × 特征数），利用卷积提取局部特征，LSTM 捕捉长期依赖，Attention 机制加权重要特征。  
   - **示例代码**：
     ```python
     from tensorflow.keras.models import Model
     from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, GlobalAveragePooling1D, Attention
     
     inputs = Input(shape=(time_steps, num_features))
     x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
     x = LSTM(units=64, return_sequences=True)(x)
     attention = Attention()([x, x])  # 自注意力机制
     x = GlobalAveragePooling1D()(attention)
     outputs = Dense(1)(x)
     
     model = Model(inputs=inputs, outputs=outputs)
     model.compile(optimizer='adam', loss='mse')
     ```

2. **XGBoost 微调**  
   - 输入为扁平化后的特征向量（样本数 × 特征数），通过决策树进一步挖掘多期信息。  
   - **示例代码**：
     ```python
     from xgboost import XGBRegressor
     
     xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)
     xgb_model.fit(X_train_flat, y_train)
     ```

3. **模型集成**  
   - 将 CNN-LSTM 和 XGBoost 的预测结果通过加权平均或 Stacking 融合。  
   - **示例代码**：
     ```python
     from sklearn.ensemble import StackingRegressor
     
     stack_model = StackingRegressor(
         estimators=[
             ('cnn_lstm', cnn_lstm_model),
             ('xgb', xgb_model)
         ],
         final_estimator=LinearRegression()
     )
     ```

---

### **3. 关键注意事项**
1. **ARIMA 预处理的必要性**  
   - **原始数据非平稳时必须进行 ARIMA 预处理**（如股票价格具有趋势性）。  
   - **平稳序列能提升模型鲁棒性**：CNN-LSTM 和 XGBoost 对非平稳数据的拟合能力有限，ARIMA 可降低噪声干扰。

2. **特征提取的多样性**  
   - **结合线性（ARIMA 残差）与非线性特征（技术指标）**：ARIMA 提取线性模式，技术指标捕捉市场行为（如 RSI 衡量超买/超卖）。  
   - **避免过度依赖单一特征**：多维度特征（如价格、成交量、技术指标）可提高模型泛化能力。

3. **参数调优与验证**  
   - **ARIMA 参数选择**：通过 ACF/PACF 图或 AIC/BIC 准则确定 `p`、`d`、`q`。  
   - **模型验证**：使用时间序列交叉验证（`TimeSeriesSplit`）避免信息泄露。  
   - **示例代码**：
     ```python
     from sklearn.model_selection import TimeSeriesSplit
     
     tscv = TimeSeriesSplit(n_splits=5)
     for train_index, test_index in tscv.split(X):
         X_train, X_test = X[train_index], X[test_index]
         y_train, y_test = y[train_index], y[test_index]
     ```

---

### **4. 总结**
| **步骤**          | **操作**                                                     | **工具/方法**                         |
| ----------------- | ------------------------------------------------------------ | ------------------------------------- |
| **1. 数据预处理** | 去除异常值、标准化、ARIMA 差分生成平稳序列                   | `StandardScaler`, `statsmodels.ARIMA` |
| **2. 特征提取**   | 提取技术指标（RSI、MACD）、统计特征（滚动均值）、结合 ARIMA 残差 | `ta`, `pandas.rolling`                |
| **3. 模型训练**   | CNN-LSTM 编码时间序列，XGBoost 微调多期信息，Stacking 集成预测结果 | `TensorFlow`, `XGBoost`, `sklearn`    |

**最终建议**：  
- **先进行 ARIMA 预处理**，将原始数据转换为平稳序列，再结合技术指标和统计特征进行提取。  
- **确保 ARIMA 残差与原始数据拼接后输入模型**，充分发挥线性与非线性特征的互补性。