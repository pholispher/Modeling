{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c8257e-1d82-462b-baa8-93fd6288a988",
   "metadata": {},
   "source": [
    "# 处理缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829614f0-51da-43dc-a6ab-dd8a0dab81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdb9e15-57d1-4638-abd9-f3f0f01aba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改为您的实际路径\n",
    "RAW_DATA_PATH = \"/root/autodl-tmp/unzipped_data/\"  # 原始CSV文件夹路径\n",
    "PROCESSED_PATH = \"./data_clean/\"  # 处理结果保存路径\n",
    "SCALERS_PATH = \"./scalers/\"  # 标准化器保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06c93c8-e675-41ef-98c2-761c0470bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# RAW_DATA_PATH = \"/root/autodl-tmp/unzipped_data/\"\n",
    "# PROCESSED_PATH = \"data/processed/\"\n",
    "\n",
    "def merge_csv_files():\n",
    "    if os.path.exists(f\"{PROCESSED_PATH}merged_data.csv\"):\n",
    "        print(\"✅ 已存在合并文件，跳过合并步骤\")\n",
    "        return pd.read_csv(f\"{PROCESSED_PATH}merged_data.csv\")\n",
    "    \n",
    "    print(\"🔄 正在合并CSV文件...\")\n",
    "    file_paths = sorted(glob(f\"{RAW_DATA_PATH}*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        raise FileNotFoundError(f\"路径 {RAW_DATA_PATH} 下无CSV文件！\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        date_str = os.path.basename(file).replace(\".csv\", \"\")\n",
    "        date_str_formatted = pd.to_datetime(date_str, format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 提取时间部分并拼接\n",
    "        df['datetime'] = date_str_formatted + \" \" + df['datetime'].str.split().str[1]\n",
    "        \n",
    "        # 明确指定格式并处理无效值\n",
    "        df['datetime'] = pd.to_datetime(\n",
    "            df['datetime'], \n",
    "            format='%Y-%m-%d %H:%M:%S',\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(f\"{PROCESSED_PATH}merged_data.csv\", index=False)\n",
    "    print(f\"✅ 合并完成，已保存至 {PROCESSED_PATH}merged_data.csv\")\n",
    "    return merged_df\n",
    "\n",
    "#df = merge_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6475c641-4751-466f-a913-613d295b5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_clean/\n"
     ]
    }
   ],
   "source": [
    "print(PROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918753f7-e686-494a-ac49-057bc8994502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在合并文件，跳过合并步骤\n"
     ]
    }
   ],
   "source": [
    "df = merge_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90e0887-ea78-47e2-8a66-86849abbd341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在缺失值处理结果，跳过该步骤\n",
      "CPU times: user 2.93 s, sys: 611 ms, total: 3.55 s\n",
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def handle_missing_values(df):\n",
    "    if os.path.exists(f\"{PROCESSED_PATH}cleaned_missing.csv\"):\n",
    "        print(\"✅ 已存在缺失值处理结果，跳过该步骤\")\n",
    "        return pd.read_csv(f\"{PROCESSED_PATH}cleaned_missing.csv\")\n",
    "    \n",
    "    print(\"🔄 处理缺失值...\")\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(f\"{PROCESSED_PATH}cleaned_missing.csv\", index=False)\n",
    "    print(f\"✅ 处理完成，已保存至 {PROCESSED_PATH}cleaned_missing.csv\")\n",
    "    return df\n",
    "\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42aa74ab-8be1-4f25-97b7-eb13a1117d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在去重结果，跳过该步骤\n",
      "CPU times: user 791 ms, sys: 153 ms, total: 943 ms\n",
      "Wall time: 941 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def remove_duplicates(df):\n",
    "    if os.path.exists(f\"{PROCESSED_PATH}cleaned_duplicates.csv\"):\n",
    "        print(\"✅ 已存在去重结果，跳过该步骤\")\n",
    "        return pd.read_csv(f\"{PROCESSED_PATH}cleaned_duplicates.csv\")\n",
    "    \n",
    "    print(\"🔄 去除重复值...\")\n",
    "    df.drop_duplicates(subset=['datetime'], keep='last', inplace=True)\n",
    "    df.to_csv(f\"{PROCESSED_PATH}cleaned_duplicates.csv\", index=False)\n",
    "    print(f\"✅ 去重完成，已保存至 {PROCESSED_PATH}cleaned_duplicates.csv\")\n",
    "    return df\n",
    "\n",
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7dbe6f-cfc0-4ae0-b252-ce667656220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在价格归一化结果，跳过该步骤\n",
      "CPU times: user 964 ms, sys: 95.1 ms, total: 1.06 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def normalize_prices(df):\n",
    "    if os.path.exists(f\"{PROCESSED_PATH}normalized_price.csv\"):\n",
    "        print(\"✅ 已存在价格归一化结果，跳过该步骤\")\n",
    "        return pd.read_csv(f\"{PROCESSED_PATH}normalized_price.csv\")\n",
    "    \n",
    "    print(\"🔄 价格归一化处理...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    price_cols = ['open', 'high', 'low', 'close']\n",
    "    df[price_cols] = scaler.fit_transform(df[price_cols])\n",
    "    \n",
    "    # 保存标准化器\n",
    "    os.makedirs(SCALERS_PATH, exist_ok=True)\n",
    "    with open(f\"{SCALERS_PATH}scaler_price.pkl\", 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    df.to_csv(f\"{PROCESSED_PATH}normalized_price.csv\", index=False)\n",
    "    print(f\"✅ 价格归一化完成，已保存至 {PROCESSED_PATH}normalized_price.csv\")\n",
    "    return df\n",
    "\n",
    "df = normalize_prices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70120937-d852-408b-a9b9-f12c86b7f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在成交量标准化结果，跳过该步骤\n",
      "CPU times: user 1.15 s, sys: 79.9 ms, total: 1.23 s\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def normalize_volumes(df):\n",
    "    if os.path.exists(f\"{PROCESSED_PATH}normalized_vol.csv\"):\n",
    "        print(\"✅ 已存在成交量标准化结果，跳过该步骤\")\n",
    "        return pd.read_csv(f\"{PROCESSED_PATH}normalized_vol.csv\")\n",
    "    \n",
    "    print(\"🔄 成交量标准化处理...\")\n",
    "    scaler = StandardScaler()\n",
    "    vol_cols = ['volume', 'openinterest']\n",
    "    df[vol_cols] = scaler.fit_transform(df[vol_cols])\n",
    "    \n",
    "    # 保存标准化器\n",
    "    with open(f\"{SCALERS_PATH}scaler_vol.pkl\", 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    df.to_csv(f\"{PROCESSED_PATH}normalized_vol.csv\", index=False)\n",
    "    print(f\"✅ 成交量标准化完成，已保存至 {PROCESSED_PATH}normalized_vol.csv\")\n",
    "    return df\n",
    "\n",
    "df = normalize_volumes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6bc0d4-c5d9-4cf9-9216-f8f62c375a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>contract</th>\n",
       "      <th>symbol</th>\n",
       "      <th>exchange</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>openinterest</th>\n",
       "      <th>volume</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03 09:00:00</td>\n",
       "      <td>SM1705</td>\n",
       "      <td>SM</td>\n",
       "      <td>CZCE</td>\n",
       "      <td>0.254069</td>\n",
       "      <td>0.249790</td>\n",
       "      <td>0.253348</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>-0.471365</td>\n",
       "      <td>-0.255511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-03 09:01:00</td>\n",
       "      <td>SM1705</td>\n",
       "      <td>SM</td>\n",
       "      <td>CZCE</td>\n",
       "      <td>0.253045</td>\n",
       "      <td>0.249035</td>\n",
       "      <td>0.254378</td>\n",
       "      <td>0.253301</td>\n",
       "      <td>-0.471307</td>\n",
       "      <td>-0.276446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03 09:02:00</td>\n",
       "      <td>SM1705</td>\n",
       "      <td>SM</td>\n",
       "      <td>CZCE</td>\n",
       "      <td>0.253642</td>\n",
       "      <td>0.249790</td>\n",
       "      <td>0.255151</td>\n",
       "      <td>0.254069</td>\n",
       "      <td>-0.471259</td>\n",
       "      <td>-0.275214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-03 09:03:00</td>\n",
       "      <td>SM1705</td>\n",
       "      <td>SM</td>\n",
       "      <td>CZCE</td>\n",
       "      <td>0.254410</td>\n",
       "      <td>0.251720</td>\n",
       "      <td>0.255580</td>\n",
       "      <td>0.255946</td>\n",
       "      <td>-0.471302</td>\n",
       "      <td>-0.276035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-03 09:04:00</td>\n",
       "      <td>SM1705</td>\n",
       "      <td>SM</td>\n",
       "      <td>CZCE</td>\n",
       "      <td>0.255946</td>\n",
       "      <td>0.252223</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>0.256117</td>\n",
       "      <td>-0.471355</td>\n",
       "      <td>-0.262900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime contract symbol exchange      open      high       low  \\\n",
       "0  2017-01-03 09:00:00   SM1705     SM     CZCE  0.254069  0.249790  0.253348   \n",
       "1  2017-01-03 09:01:00   SM1705     SM     CZCE  0.253045  0.249035  0.254378   \n",
       "2  2017-01-03 09:02:00   SM1705     SM     CZCE  0.253642  0.249790  0.255151   \n",
       "3  2017-01-03 09:03:00   SM1705     SM     CZCE  0.254410  0.251720  0.255580   \n",
       "4  2017-01-03 09:04:00   SM1705     SM     CZCE  0.255946  0.252223  0.257040   \n",
       "\n",
       "      close  openinterest    volume  amount  \n",
       "0  0.252874     -0.471365 -0.255511       0  \n",
       "1  0.253301     -0.471307 -0.276446       0  \n",
       "2  0.254069     -0.471259 -0.275214       0  \n",
       "3  0.255946     -0.471302 -0.276035       0  \n",
       "4  0.256117     -0.471355 -0.262900       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497a250-3bd1-4470-be5e-c97c7e7bb1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
